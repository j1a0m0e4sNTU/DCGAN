        
ID: tutorial 
 infomation: folow the tutorial 
 Generator: g_tutorial 
 Discriminator: d_tutorial 
 Learning rate: 0.002 
 Epoch number: 500 
 Batch size: 64 
 =======================

Generator(
  (model): Sequential(
    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)
    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace)
    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(inplace)
    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): ReLU(inplace)
    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (11): ReLU(inplace)
    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (13): Tanh()
  )
)
Discriminator(
  (model): Sequential(
    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LeakyReLU(negative_slope=0.2, inplace)
    (3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): LeakyReLU(negative_slope=0.2, inplace)
    (6): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): LeakyReLU(negative_slope=0.2, inplace)
    (9): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (10): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (11): LeakyReLU(negative_slope=0.2, inplace)
    (12): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)
    (13): Sigmoid()
  )
)
 Epoch  0 Step 100 | G loss : 0.04018466919660568 | D loss (real) :  0.017190150916576385 | D loss (fake) :  0.008489048108458519
 Epoch  0 Step 200 | G loss : 0.00902626570314169 | D loss (real) :  0.013806913048028946 | D loss (fake) :  0.006656385958194733
 Epoch  0 Step 300 | G loss : 0.04391781613230705 | D loss (real) :  0.007716509513556957 | D loss (fake) :  0.011568780988454819
 Epoch  0 Step 400 | G loss : 0.023544996976852417 | D loss (real) :  0.015083279460668564 | D loss (fake) :  0.004860736429691315
 Epoch  0 Step 500 | G loss : 0.02723793499171734 | D loss (real) :  0.012980220839381218 | D loss (fake) :  0.006813753861933947
 Epoch  0 Step 600 | G loss : 0.036712683737277985 | D loss (real) :  0.013996923342347145 | D loss (fake) :  0.01059197448194027
 Epoch  1 Step 100 | G loss : 0.018550489097833633 | D loss (real) :  0.009439535439014435 | D loss (fake) :  0.008615332655608654
 Epoch  1 Step 200 | G loss : 0.03192397579550743 | D loss (real) :  0.012292430736124516 | D loss (fake) :  0.004523825366050005
 Epoch  1 Step 300 | G loss : 0.03875679522752762 | D loss (real) :  0.008458390831947327 | D loss (fake) :  0.011456510052084923
 Epoch  1 Step 400 | G loss : 0.041430942714214325 | D loss (real) :  0.007833299227058887 | D loss (fake) :  0.013010704889893532
 Epoch  1 Step 500 | G loss : 0.024421431124210358 | D loss (real) :  0.013539860025048256 | D loss (fake) :  0.005813955329358578
 Epoch  1 Step 600 | G loss : 0.03141782805323601 | D loss (real) :  0.013091612607240677 | D loss (fake) :  0.0075980136170983315
 Epoch  2 Step 100 | G loss : 0.03144504874944687 | D loss (real) :  0.007258855737745762 | D loss (fake) :  0.009802058339118958
 Epoch  2 Step 200 | G loss : 0.026336636394262314 | D loss (real) :  0.014765158295631409 | D loss (fake) :  0.004595955368131399
 Epoch  2 Step 300 | G loss : 0.07734736800193787 | D loss (real) :  0.005289001390337944 | D loss (fake) :  0.014078105799853802
 Epoch  2 Step 400 | G loss : 0.049790751188993454 | D loss (real) :  0.003870653221383691 | D loss (fake) :  0.019816825166344643
 Epoch  2 Step 500 | G loss : 0.04968130588531494 | D loss (real) :  0.003693878883495927 | D loss (fake) :  0.011101128533482552
 Epoch  2 Step 600 | G loss : 0.022227730602025986 | D loss (real) :  0.023077189922332764 | D loss (fake) :  0.000946104759350419
 Epoch  3 Step 100 | G loss : 0.04708420857787132 | D loss (real) :  0.007277966942638159 | D loss (fake) :  0.00637234840542078
 Epoch  3 Step 200 | G loss : 0.03698030114173889 | D loss (real) :  0.013055267743766308 | D loss (fake) :  0.0035875048488378525
 Epoch  3 Step 300 | G loss : 0.07236557453870773 | D loss (real) :  0.004577994346618652 | D loss (fake) :  0.01744973100721836
 Epoch  3 Step 400 | G loss : 0.06311844289302826 | D loss (real) :  0.008247744292020798 | D loss (fake) :  0.01340012438595295
 Epoch  3 Step 500 | G loss : 0.07328179478645325 | D loss (real) :  0.0016791382804512978 | D loss (fake) :  0.01829877868294716
 Epoch  3 Step 600 | G loss : 0.04188239201903343 | D loss (real) :  0.01037835143506527 | D loss (fake) :  0.003580726683139801
 Epoch  4 Step 100 | G loss : 0.04262085258960724 | D loss (real) :  0.005689964629709721 | D loss (fake) :  0.008253403007984161
 Epoch  4 Step 200 | G loss : 0.05471929907798767 | D loss (real) :  0.005396984983235598 | D loss (fake) :  0.012784991413354874
 Epoch  4 Step 300 | G loss : 0.04606505483388901 | D loss (real) :  0.0040538921020925045 | D loss (fake) :  0.009013715200126171
 Epoch  4 Step 400 | G loss : 0.03686888515949249 | D loss (real) :  0.009807515889406204 | D loss (fake) :  0.003671304089948535
 Epoch  4 Step 500 | G loss : 0.02824316918849945 | D loss (real) :  0.011255575343966484 | D loss (fake) :  0.005261917598545551
 Epoch  4 Step 600 | G loss : 0.021319953724741936 | D loss (real) :  0.00987478718161583 | D loss (fake) :  0.005542326718568802
 Epoch  5 Step 100 | G loss : 0.04092175513505936 | D loss (real) :  0.006243880372494459 | D loss (fake) :  0.0049959938041865826
 Epoch  5 Step 200 | G loss : 0.015285441651940346 | D loss (real) :  0.017849287018179893 | D loss (fake) :  0.002016638172790408
 Epoch  5 Step 300 | G loss : 0.020425651222467422 | D loss (real) :  0.020185567438602448 | D loss (fake) :  0.002456200774759054
 Epoch  5 Step 400 | G loss : 0.05703992024064064 | D loss (real) :  0.001925047254189849 | D loss (fake) :  0.023388052359223366
 Epoch  5 Step 500 | G loss : 0.022857535630464554 | D loss (real) :  0.017992980778217316 | D loss (fake) :  0.0018515472766011953
 Epoch  5 Step 600 | G loss : 0.033405013382434845 | D loss (real) :  0.008837942034006119 | D loss (fake) :  0.004745871759951115
 Epoch  6 Step 100 | G loss : 0.02028212696313858 | D loss (real) :  0.015380946919322014 | D loss (fake) :  0.0036846939474344254
 Epoch  6 Step 200 | G loss : 0.045761995017528534 | D loss (real) :  0.006186135578900576 | D loss (fake) :  0.010752785950899124
 Epoch  6 Step 300 | G loss : 0.023582521826028824 | D loss (real) :  0.013979315757751465 | D loss (fake) :  0.006503095384687185
 Epoch  6 Step 400 | G loss : 0.016534609720110893 | D loss (real) :  0.017238225787878036 | D loss (fake) :  0.0028203455731272697
 Epoch  6 Step 500 | G loss : 0.05715093016624451 | D loss (real) :  0.0036176403518766165 | D loss (fake) :  0.012428118847310543
 Epoch  6 Step 600 | G loss : 0.04686289280653 | D loss (real) :  0.005310188978910446 | D loss (fake) :  0.006393722258508205
 Epoch  7 Step 100 | G loss : 0.07873693108558655 | D loss (real) :  0.0016315364046022296 | D loss (fake) :  0.02386537194252014
 Epoch  7 Step 200 | G loss : 0.05905546247959137 | D loss (real) :  0.0042503830045461655 | D loss (fake) :  0.012162809260189533
 Epoch  7 Step 300 | G loss : 0.027908064424991608 | D loss (real) :  0.01120930165052414 | D loss (fake) :  0.0038331178948283195
 Epoch  7 Step 400 | G loss : 0.034585218876600266 | D loss (real) :  0.0074863797053694725 | D loss (fake) :  0.0037826846819370985
